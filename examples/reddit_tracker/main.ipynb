{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Reddit tracker\n",
    "\n",
    "This notebook details:  \n",
    "1. Setting up a Reddit API wrapper and querying Reddit\n",
    "2. Extracting names from Reddit submission titles with SpaCy\n",
    "3. Saving the results as a dataframe\n",
    "4. Counting the top mentioned names and plotting them with Plotly\n",
    "5. Saving the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import time\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'praw = {praw.__version__}')\n",
    "print(f'pandas = {pd.__version__}')\n",
    "print(f'spacy = {spacy.__version__}')\n",
    "from platform import python_version\n",
    "print(f'python version = {python_version()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Python Reddit API Wrapper (PRAW)\n",
    "You need a user account - it's fine to create a brand new one.\n",
    "\n",
    "A really useful library that allows querying of Reddit's API in python.  \n",
    "Check the documentation and follow the steps to generate your app client and find your client ID and client Secret  \n",
    "https://praw.readthedocs.io/en/latest/getting_started/quick_start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Secrets with treebeard\n",
    "\n",
    "I want to save my reddit credentials separately from my code and this notebook.  \n",
    "Create a JSON file with a dictionary of the reddit app secrets, and then upload with `treebeard secrets push secrets.json`  \n",
    "Ignore the file in `.gitignore` and `treebeard.yaml` so it does not get stored with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't save your creds in code! This is an example just to show how to construct the JSON file.\n",
    "my_creds = {'username': 'my_username', \n",
    "            'password': 'my_password', \n",
    "            'app_client_id': 'app_client_ID', \n",
    "            'app_client_secret': 'app_client_secret'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dummy_secrets.json', 'w') as f:\n",
    "    f.write(json.dumps(my_creds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dummy_secrets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !treebeard secrets push dummy_secrets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures secrets are available across any cloud project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pushing my real credentials file locally\n",
    "# !treebeard secrets push secrets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Workflow for usual use with credentials\n",
    "# =========================================\n",
    "\n",
    "with open('secrets.json', 'r') as f:\n",
    "    secrets = json.loads(f.read())\n",
    "    \n",
    "# # Reddit Credentials - add your own by following the steps in the quick start link above.\n",
    "username = secrets['username']\n",
    "password = secrets['password']\n",
    "app_client_id = secrets['app_client_id']\n",
    "app_client_secret = secrets['app_client_secret']\n",
    "user_agent = \"script:my_app:v0.1 (by u/laurence_treebeard)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=app_client_id,\n",
    "                     client_secret=app_client_secret,\n",
    "                     user_agent=user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we have a reddit read_only instance\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we can query reddit\n",
    "# options: controversial, gilded, hot, new, rising, top\n",
    "for submission in reddit.subreddit('learnpython').hot(limit=5):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now queried the Reddit API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SpaCy\n",
    "\n",
    "SpaCy can recognise a range of named entities:  \n",
    "https://spacy.io/api/annotation#named-entities  \n",
    "- PERSON\tPeople, including fictional.\n",
    "- NORP\tNationalities or religious or political groups.\n",
    "- FAC\tBuildings, airports, highways, bridges, etc.\n",
    "- ORG\tCompanies, agencies, institutions, etc.\n",
    "- GPE\tCountries, cities, states.\n",
    "- LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART\tTitles of books, songs, etc.\n",
    "- LAW\tNamed documents made into laws.\n",
    "- LANGUAGE\tAny named language.\n",
    "- DATE\tAbsolute or relative dates or periods.\n",
    "- TIME\tTimes smaller than a day.\n",
    "- PERCENT\tPercentage, including ”%“.\n",
    "- MONEY\tMonetary values, including unit.\n",
    "- QUANTITY\tMeasurements, as of weight or distance.\n",
    "- ORDINAL\t“first”, “second”, etc.\n",
    "- CARDINAL\tNumerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy's text model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SpaCy and render the output - shiny!\n",
    "doc = nlp(\"Apple is looking at buying a U.K. startup for $1 billion\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this into a helpful function for later\n",
    "def spacy_extract(df, label = 'PERSON'):\n",
    "    '''\n",
    "    Takes a pandas Dataframe object and a named entity label\n",
    "    Returns an array of arrays for each reddit submission\n",
    "    in the dataframe.\n",
    "    '''\n",
    "    titles = df['title']\n",
    "    output = [] # output\n",
    "    for title in titles:\n",
    "        names = [] \n",
    "        title = title.replace(\"'s\", \"\") # clear out apostrophe's\n",
    "        doc = nlp(title)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == label:\n",
    "                names.append(ent.text)\n",
    "        output.append(names)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Reddit\n",
    "Input types:  \n",
    "- subreddit\n",
    "- new or hot or top\n",
    "- entity types: person, organisation, organisation, product etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out a query\n",
    "# Try replacing top with new - but you'll have to remove the time_filter as it always takes the most recent\n",
    "df = pd.DataFrame([[x.title, x.score, x.id, x.url] for x in reddit.subreddit('nba').top(limit=100, time_filter='week')], columns=['title', 'score', 'id', 'url'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SpaCy on Reddit submission titles\n",
    "for x in df['title'][:10]:\n",
    "    doc = nlp(x)\n",
    "    displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    ''' \n",
    "    Given an input dataframe plot a horizontal bar chart\n",
    "    df: pandas dataframe with two columns, 'Names' and 'Count'\n",
    "    look_for: string, SpaCy entity type\n",
    "    subreddit: string, name of subreddit\n",
    "    '''\n",
    "    df = df[:20].iloc[::-1].reset_index()\n",
    "    fig = px.bar(df, x=\"Count\", y=\"Names\", orientation='h')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(subreddit='news', look_for='PERSON', sort='top', limit=100, time_filter='month'):\n",
    "    '''\n",
    "    Sort can be 'top' or 'hot' or 'new'\n",
    "    Limit should be max 1000\n",
    "    Time_filter can be 'hour', day', 'week', 'month', 'year', 'all'\n",
    "    If 'new' is selected, time_filter is unused\n",
    "    '''\n",
    "    if limit > 1000:\n",
    "        print('Limit should be less than or equal to 1000')\n",
    "        return\n",
    "    \n",
    "    time_filters = ['hour', 'day', 'week', 'month', 'year', 'all']\n",
    "    if time_filter not in time_filters:\n",
    "        print(f'Incorrect time filter. Expecting one of {time_filters}')\n",
    "    \n",
    "    columns = ['title', 'score', 'id', 'url', 'datetime']\n",
    "    if sort=='hot':\n",
    "        df = pd.DataFrame([[x.title, x.score, x.id, x.url, datetime.fromtimestamp(x.created_utc, timezone.utc)] \\\n",
    "                           for x in reddit.subreddit(subreddit).hot(limit=limit, time_filter=time_filter)],\n",
    "                    columns=columns)\n",
    "    elif sort=='new':\n",
    "        df = pd.DataFrame([[x.title, x.score, x.id, x.url, datetime.fromtimestamp(x.created_utc, timezone.utc)] \\\n",
    "                           for x in reddit.subreddit(subreddit).new(limit=limit)],\n",
    "                  columns=columns)\n",
    "    else:\n",
    "        df = pd.DataFrame([[x.title, x.score, x.id, x.url, datetime.fromtimestamp(x.created_utc, timezone.utc)] \\\n",
    "                           for x in reddit.subreddit(subreddit).top(limit=limit, time_filter=time_filter)],\n",
    "              columns=columns)\n",
    "        \n",
    "    print(f'{len(df)} submissions found')\n",
    "    print(f'Extracting {look_for}s')\n",
    "    df['data'] = spacy_extract(df, label=look_for)\n",
    "    \n",
    "    # Return top 10\n",
    "    flat_list = [item for sublist in df['data'] for item in sublist]\n",
    "    c = Counter(flat_list)\n",
    "    top = pd.DataFrame(c.most_common(), columns=['Names', 'Count'])\n",
    "    \n",
    "    chart = plot(top) # plot chart\n",
    "    return df, top, chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "df, top, chart = get_subreddit(subreddit='nba', look_for='PERSON', sort='top', limit=100, time_filter='week')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_to_query = [\"soccer\",\"baseball\",\"hockey\",\"mma\",\"running\",\"snowboarding\",\n",
    "                       \"climbing\",\"nba\",\"nfl\",\"politics\",\"casualuk\",\"news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure there is a local directory to save images in\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the images locally\n",
    "for subreddit in subreddits_to_query:\n",
    "    df, top, chart = get_subreddit(subreddit=subreddit, look_for='PERSON', sort='top', limit=999, time_filter='week')\n",
    "    name = f\"{subreddit}.html\"\n",
    "    chart.write_html(f'output/{name}') # save image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
